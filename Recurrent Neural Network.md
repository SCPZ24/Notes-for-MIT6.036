# Components
RNN（循环神经网络）也可以理解为一个状态机。
假设讨论第$t$轮，其state是一个$m×1$向量$s_t$。
输入向量$x_t$，可以认为是$1×1(Special Case)$，也可以认为是$c×1$。
预测向量$p_t$，为$v×1$，为一个链结的输出值。
线性操作的矩阵
	外界输入向量$x_t$前面的系数矩阵$W^{sx}$，为$m×c$
	上一轮迭代向量$s_{t-1}$前面的系数矩阵$W^{ss}$，为$m×m$
	常数矩阵$W_0^{ss}$，为$m×1$
	获取$p_t$的激活函数（第二次激活函数）前的线性变换矩阵$W^o(v×m)与W_0^o(v×1)$
![[循环神经网络结构示意图.png]]
# Operations for Each Layer
已知输入向量$x_t$，状态向量$s_{t-1}$。
则线性变换有
$$
z_t^1=W^{sx}x_t+W^{ss}s_{t-1}+W_0^{ss}
$$
转入激活函数$f_1$得$s_t$
$$
s_t=f_1(z_t^1)
$$
这个$s_t$即为下一层的状态向量
做第二次线性变换
$$
z_t^2=W^os_t+W_0^o
$$
经过激活函数得到这一层向外界输出的预测向量
$$
p_t=f_2(z_t^2)
$$
注意变元右上角的数字并不是指数而是上标。
举例：假设在做一个英文字母预测提词器，认为状态机中保存已经输入的3个英文字符为$s_{t-1}$，现在再从字符串中读取下一个字符$x_t$。我们要使得$s_t$中有$s_{t-1}$中的后两个字符以及$x_t$，（字符可以做One-Hot编码，但是这里假设是一位线性的方便表示）那我们可以认为$f_1$为$f_1(x)=x$，然后适当的取转换矩阵。
$$
s_t = \begin{bmatrix}1\\0\\0\end{bmatrix}x_t+\begin{bmatrix}0\,\,0\,\,0\\1\,\,0\,\,0\\0\,\,1\,\,0\end{bmatrix}s_{t-1}
$$
实际情况更加复杂，但是参数训练完后差不多会是如是情形。
# Loss and Gradient
RNN的训练样本应该是有时序的一串序列，比如字符串。
每次迭代序列的下一列或下一子串$x_t$进入神经网络。
我们假设训练样本有$q$个序列。每个序列有$n^{(i)}$个子单元。
则一个序列的损失为
$$
L_{seq}(p^{(i)},y^{(i)})=\sum_{t=1}^{n^{(i)}}L_{elt}(p_t^{(i)},y_t^{(i)})
$$
总损失为
$$
J(W^o,W_0^o)=\sum_{i=1}^qL_{seq}(p^{(i)},y^{(i)})
$$
对一个$L_{seq}$，其对于参数的梯度有（以$W^{sx}$）为例
$$
\frac{\partial L_{seq}(p^{(i)},y^{(i)})}{\partial W^{sx}}=\sum_{t=1}^{n^{(i)}}\frac{\partial L_{elt}(p_t^{(i)},y_t^{(i)})}{\partial W^{sx}}
$$
