# Encode Data in Usable Form(Featurization)
有时，数据集中的数据并不可以直接变成可以直接变成数字的形式用于分析。因此需要一些其他标的对数据进行预处理。
这些预处理本身就是一种映射（Mapping）。把原来的数据$x(比如区间、字符串)$映射成可以用于线性化数据训练的数据$\phi(x)$。
下以分析心脏病发病的情况为例阐述如何把数据预处理。
![[心脏病样本调查表.png]]
## 0-1Mapping
图中第一列“Has heart disease”中只有是或者否，则可以用数据1表示是，0表示否。由此参与学习模型。
## One-hot Encoding
图中“Job”栏无法用单一数据表述两者的区别。
因为如果给每个职业都赋一个值：护士-1，高管-2，医生-3。依据线性运算的理念，可以认为“高管介于护士和医生之间”。但没有任何明确标的来对这些职业做排序。
所以我们可以采取升维，把所有职业列入一张表。
![[单一热点编码示意图.png]]
如上图。假设一共有5个职业，则把表示职业的列扩维成5列。每个职业只在对应的维度为1。
## Factored Encoding
对于上图中的药物栏，如果采取One-hot Encoding，则需要用到$2^2=4$维(p,p&b,b,null)。并且无法体现p与p&b两组中共同用了p药物这样的关系。
因此可以用Factored Encoding：药物有几种，就用几维。
![[要素导向编码示意图.png]]
## Using a Representative for a Range
表中的年龄，照理可以用一个年龄段的中间数来表示一个区间。
然而实际上，对于一个表示区间的数据，有时候用区间的中间值（即为每个区间给予了默认值）其实有失偏颇。
因此需要用一个代表数据来表示一个区间。
因此，可以把表中的20s映射为2,40s映射为4，50s映射为5。
## Unary/Thermometer Code
有些数据之间有程度关联但是无法定义明确的线性关系（Ordinal Data）。
比如强烈反对-反对-中立-同意-强烈同意之间，有程度区别但是没法定义每种态度之间的心理差别是一样的（各种态度的关系是线性的）。有时候同意和强烈同意之间的差别大于中立和同意之间的差别。因此没法用一个数字表示。
因此可用温度计编码。
比如一共有5种态度，则可以扩维成5维。每种态度对应的列前面的列全为1，后面的全为0：
![[温度计编码示意图.png]]
# Standardize Numerical Datas
有时数字类型的数据在数量级上区别很大，对于有些学习算法来说训练十分困难。因此需要对数字类型标准化。
假设数据集中某一列数据为数据类型。记为$x_d^{(1)}，x_d^{(2)}，x_d^{(3)}，x_d^{(4)}......,x_d^{(n)}$，他们的算数平均值为$\overline{x}$,标准差为σ。
则对一个数据，标准化后为
$$
\phi_d^{(k)}=\frac{x_d^{(k)}-\overline{x}}{σ}
$$
# Processing of Nonlinear Boundaries
![[非线性边缘数据集举例.png]]
有些数据集不同区域的边缘并非直线（如左下角图）。这就需要引入Taylor Polynomial（泰勒多项式）。
分离器的本质：对于一个d元函数$z=f(x_1,x_2......,x_d)+h_0$，
	若满足$z>0$，则判定为y=+1
	若满足$z<0$，则判定为y=-1
线性分离器的$f(x_1,x_2......,x_d)$满足对于所有变量，都是一次的。
其他的函数不一定时一次的，但我们知道如果如果对函数泰勒展开，则一定会形成一个多项式的形式。
我们取所有次数低于a的项，那这些项里包含

| 次数     | 项                                                                          |
| ------ | -------------------------------------------------------------------------- |
| 0      | $h_0$                                                                      |
| 1      | $x_1,x_2,x_3......,x_d$                                                    |
| 2      | $x_1^2,x_2^2,x_3^2......,x_d^2,x_1x_2,x_1x_3......x_1x_d,......x_{d-1}x_d$ |
| 3      | 所有三次项（包括一个变量的立方，两个变量一个一次一个两次，三个变量各一次）                                      |
| ...... | ......                                                                     |
| a      | 同上                                                                         |
依照上面的展开方法，把一个样本的上述的所有的项放在一个新的向量里形成一个新的样本。
这个样本其实维度还是d，但是向量长度比d长。因为只需确定$x_1,x_2,x_3......,x_d$共d个值，就可以确定整个向量。
# Detecting Overfitting（过度拟合）
## Definition of Overfitting
对于一个数据集，有时候会出现过度拟合的状况。
![[过度拟合举例.png]]
如图。图中的样本可以用线性分离器来拟合。
但是图中应该是采用了高次的拟合方法，使得把y=+1的值都极度精确的描出来了（过于依赖样本数据）。这会在实际应用中产生错误。
## Rough Ways to Detect Overfitting of a Learning Algorithm
对于一个已有的数据集，我们可以做一个分类：一部分纯用于训练，另一部分用于检测训练结果。
	一般有训练-测试用数据以3:1分开或者1:1分开。
	训练用的数据多，则越接近真实情况。
	测试用的数据多，则检测时的噪音更小。
	可以将数据集shuffle（重新排列）来减小一些极端情况（比如训练用的数据大多都y=+1，测试用的大多都y=-1）。
## More Precise Way to Evaluate a Learning Algorithm
方法：Cross-Validate(Dn,k)
	把数据随机平均分成k组（记为$D_{n,1},D_{n,2},D_{n_3}......,D_{n,k}$）。
	然后进行k论循环。在第i轮中：
		基于$D_n/D_{n,i}$这个数据集训练出一个$h_i$。其中$D_n/D_{n,i}=\begin{Bmatrix}x|x\in D_n且x\notin D_{n,i}\end{Bmatrix}$。
		基于$D_{n,i}$这个数据集计算出这一轮的平均误差$\varepsilon(h_i,D_{n,i})$。
	最后返回$Res=\frac{1}{k}\sum_{i=1}^k\varepsilon(h_i,D_{n,i})$。如果使用0-1Loss，那么这个返回值应该介于0和1之间。
	比较有没有组的平均误差与Res有明显差别。